{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version:\n",
      "2.7.12 |Anaconda 4.1.1 (64-bit)| (default, Jul  2 2016, 17:42:40) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n",
      "\n",
      "spark context version:\n",
      "1.6.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"python version:\\n{}\\n\".format(sys.version))\n",
    "\n",
    "print(\"spark context version:\\n{}\".format(sc.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark dataframes from python collections (lists) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "[('Alice', 1), ('Bob', 2)]\n",
      "\n",
      "df:\n",
      "DataFrame[name: string, age: bigint]\n",
      "[Row(name=u'Alice', age=1), Row(name=u'Bob', age=2)]\n",
      "\n",
      "df_age10:\n",
      "DataFrame[name: string, age10: bigint]\n",
      "[Row(name=u'Alice', age10=11), Row(name=u'Bob', age10=12)]\n",
      "\n",
      "df:\n",
      "DataFrame[name: string, age: bigint]\n",
      "[Row(name=u'Alice', age=1), Row(name=u'Bob', age=2)]\n",
      "\n",
      "df_drop_age:\n",
      "DataFrame[name: string]\n",
      "[Row(name=u'Alice'), Row(name=u'Bob')]\n",
      "\n",
      "df_slen:\n",
      "DataFrame[slen: int]\n",
      "[Row(slen=5), Row(slen=3)]\n",
      "\n",
      "df_doubled:\n",
      "DataFrame[name: string, age: bigint, age_doubled: int]\n",
      "[Row(name=u'Alice', age=1, age_doubled=2), Row(name=u'Bob', age=2, age_doubled=4)]\n",
      "\n",
      "df_filtered:\n",
      "DataFrame[name: string, age: bigint, age_doubled: int]\n",
      "[Row(name=u'Bob', age=2, age_doubled=4)]\n",
      "\n",
      "df2:\n",
      "DataFrame[name: string, age: bigint]\n",
      "[Row(name=u'Alice', age=1), Row(name=u'Bob', age=2), Row(name=u'Bob', age=2)]\n",
      "\n",
      "df2_distinct:\n",
      "DataFrame[name: string, age: bigint]\n",
      "[Row(name=u'Bob', age=2), Row(name=u'Alice', age=1)]\n",
      "\n",
      "df2_sorted:\n",
      "DataFrame[name: string, age: bigint]\n",
      "[Row(name=u'Bob', age=2), Row(name=u'Alice', age=1)]\n",
      "\n",
      "data3:\n",
      "[Row(a=1, intlist=[1, 2, 3])]\n",
      "\n",
      "df3:\n",
      "DataFrame[a: bigint, intlist: array<bigint>]\n",
      "[Row(a=1, intlist=[1, 2, 3])]\n",
      "\n",
      "df_explode:\n",
      "DataFrame[anInt: bigint]\n",
      "[Row(anInt=1), Row(anInt=2), Row(anInt=3)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data=[(\"Alice\",1),(\"Bob\",2)]\n",
    "print(\"data:\\n{}\\n\".format(data))\n",
    "\n",
    "df=sqlContext.createDataFrame(data,[\"name\",\"age\"])\n",
    "print(\"df:\\n{}\\n{}\\n\".format(df,df.take(10)))\n",
    "\n",
    "df_age10=df.select(df.name,(df.age+10).alias(\"age10\"))\n",
    "print(\"df_age10:\\n{}\\n{}\\n\".format(df_age10,df_age10.take(10)))\n",
    "\n",
    "df_drop_age=df.drop(df.age)\n",
    "print(\"df:\\n{}\\n{}\\n\".format(df,df.take(10)))\n",
    "print(\"df_drop_age:\\n{}\\n{}\\n\".format(df_drop_age,df_drop_age.take(10)))\n",
    "\n",
    "## User Defined Function (UDF) Transformation \n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "slen=udf(lambda s: len(s),IntegerType())\n",
    "df_slen=df_drop_age.select(slen(df_drop_age.name).alias(\"slen\"))\n",
    "print(\"df_slen:\\n{}\\n{}\\n\".format(df_slen,df_slen.take(10)))\n",
    "\n",
    "## Other useful transformations\n",
    "    ## filter(func)\n",
    "    ## where(func)\n",
    "    ## distinct()\n",
    "    ## orderBy(*cols,**kw)\n",
    "    ## sort(*cols,**kw)\n",
    "    ## explode(col) # returns a new row for each element in the given array or map\n",
    "\n",
    "doubled=udf(lambda d: d*2,IntegerType())\n",
    "df_doubled=df.select(df.name,df.age,doubled(df.age).alias(\"age_doubled\"))\n",
    "print(\"df_doubled:\\n{}\\n{}\\n\".format(df_doubled,df_doubled.take(10)))\n",
    "\n",
    "df_filtered=df_doubled.filter(df_doubled.age_doubled>3)\n",
    "print(\"df_filtered:\\n{}\\n{}\\n\".format(df_filtered,df_filtered.take(10)))\n",
    "\n",
    "data2=[(\"Alice\",1),(\"Bob\",2),(\"Bob\",2)]\n",
    "df2=sqlContext.createDataFrame(data2,[\"name\",\"age\"])\n",
    "df2_distinct=df2.distinct()\n",
    "print(\"df2:\\n{}\\n{}\\n\".format(df2,df2.take(10)))\n",
    "print(\"df2_distinct:\\n{}\\n{}\\n\".format(df2_distinct,df2_distinct.take(10)))\n",
    "\n",
    "df2_sorted=df2_distinct.sort(df2_distinct.age,ascending=False)\n",
    "print(\"df2_sorted:\\n{}\\n{}\\n\".format(df2_sorted,df2_sorted.take(10)))\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "data3=[pyspark.sql.Row(a=1,intlist=[1,2,3])]\n",
    "df3=sqlContext.createDataFrame(data3)\n",
    "df_explode=df3.select(explode(df3.intlist).alias(\"anInt\"))\n",
    "print(\"data3:\\n{}\\n\".format(data3))\n",
    "print(\"df3:\\n{}\\n{}\\n\".format(df3,df3.take(10)))\n",
    "print(\"df_explode:\\n{}\\n{}\\n\".format(df_explode,df_explode.take(10)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupedData Transformations ###\n",
    "\n",
    "+ groupBy(*cols)\n",
    "+ GroupedData functions\n",
    "    - agg(*exprs)\n",
    "        + compute aggregates (avg,max,min,sum,or count) and returns a DataFrame\n",
    "    - count()\n",
    "    - avg(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "[('Alice', 1, 6), ('Bob', 2, 8), ('Alice', 3, 9), ('Bob', 4, 7)]\n",
      "\n",
      "df:\n",
      "DataFrame[name: string, age: bigint, grade: bigint]\n",
      "[Row(name=u'Alice', age=1, grade=6), Row(name=u'Bob', age=2, grade=8), Row(name=u'Alice', age=3, grade=9), Row(name=u'Bob', age=4, grade=7)]\n",
      "\n",
      "df_agg_count:\n",
      "DataFrame[name: string, count(1): bigint]\n",
      "[Row(name=u'Alice', count(1)=2), Row(name=u'Bob', count(1)=2)]\n",
      "\n",
      "df_groupBy_count:\n",
      "DataFrame[name: string, count: bigint]\n",
      "[Row(name=u'Alice', count=2), Row(name=u'Bob', count=2)]\n",
      "\n",
      "df_groupBy_avg:\n",
      "DataFrame[avg(age): double, avg(grade): double]\n",
      "[Row(avg(age)=2.5, avg(grade)=7.5)]\n",
      "\n",
      "df_groupBy_name_avg:\n",
      "DataFrame[name: string, avg(age): double, avg(grade): double]\n",
      "[Row(name=u'Alice', avg(age)=2.0, avg(grade)=7.5), Row(name=u'Bob', avg(age)=3.0, avg(grade)=7.5)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(\"Alice\",1,6),(\"Bob\",2,8),(\"Alice\",3,9),(\"Bob\",4,7)]\n",
    "df=sqlContext.createDataFrame(data,[\"name\",\"age\",\"grade\"])\n",
    "df_agg_count=df.groupBy(df.name).agg({\"*\":\"count\"})\n",
    "df_groupBy_count=df.groupBy(df.name).count()\n",
    "df_groupBy_avg=df.groupBy().avg()\n",
    "df_groupBy_name_avg=df.groupBy(df.name).avg()\n",
    "print(\"data:\\n{}\\n\".format(data))\n",
    "print(\"df:\\n{}\\n{}\\n\".format(df,df.take(10)))\n",
    "print(\"df_agg_count:\\n{}\\n{}\\n\".format(df_agg_count,df_agg_count.take(10)))\n",
    "print(\"df_groupBy_count:\\n{}\\n{}\\n\".format(df_groupBy_count,df_groupBy_count.take(10)))\n",
    "print(\"df_groupBy_avg:\\n{}\\n{}\\n\".format(df_groupBy_avg,df_groupBy_avg.take(10)))\n",
    "print(\"df_groupBy_name_avg:\\n{}\\n{}\\n\".format(df_groupBy_name_avg,df_groupBy_name_avg.take(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Actions ###\n",
    "\n",
    "+ show(n,truncate)\n",
    "    - prints the first n rows of the DataFrame\n",
    "+ take(n)\n",
    "    - returns the first n rows of a list of Row\n",
    "+ collect()\n",
    "    - returns all records of a list of Row\n",
    "+ count()\n",
    "    - count for DataFrame is an action, while for GroupedData it is a transformation\n",
    "+ describe(*cols*)\n",
    "    - Exploratory Data Analysis function that computes statistics (count,mean,stddev,min,max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collect():\n",
      "[Row(name=u'Alice', age=1), Row(name=u'Bob', age=2)]\n",
      "\n",
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Alice|  1|\n",
      "|  Bob|  2|\n",
      "+-----+---+\n",
      "\n",
      "show():\n",
      "None\n",
      "\n",
      "count():\n",
      "2\n",
      "\n",
      "take():\n",
      "[Row(name=u'Alice', age=1)]\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|               age|\n",
      "+-------+------------------+\n",
      "|  count|                 2|\n",
      "|   mean|               1.5|\n",
      "| stddev|0.7071067811865476|\n",
      "|    min|                 1|\n",
      "|    max|                 2|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(\"Alice\",1),(\"Bob\",2)]\n",
    "df=sqlContext.createDataFrame(data,[\"name\",\"age\"])\n",
    "print(\"collect():\\n{}\\n\".format(df.collect()))\n",
    "print(\"count():\\n{}\\n\".format(df.count()))\n",
    "print(\"take():\\n{}\\n\".format(df.take(1)))\n",
    "print(\"show():\\n{}\\n\".format(df.show()))\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark DataFrames using Python Pandas DataFrame ##\n",
    "\n",
    "+ [Python Pandas](http://pandas.pydata.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf:\n",
      "    name  age\n",
      "0  Alice    1\n",
      "1    Bob    2\n",
      "\n",
      "df:\n",
      "DataFrame[name: string, age: bigint]\n",
      "[Row(name=u'Alice', age=1), Row(name=u'Bob', age=2)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pdf=pd.DataFrame(data,columns=[\"name\",\"age\"])\n",
    "print(\"pdf:\\n{}\\n\".format(pdf))\n",
    "df=sqlContext.createDataFrame(pdf)\n",
    "print(\"df:\\n{}\\n{}\".format(df,df.take(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark dataframe using HDFS file ###\n",
    "\n",
    "+ create README.md file in local dir.\n",
    "\n",
    "+ put README.md into hdfs\n",
    "\n",
    "    - $ hdfs dfs -ls /user/cloudera\n",
    "\n",
    "    - $ hdfs dfs -mkdir /user/cloudera/edx\n",
    "\n",
    "    - $ hdfs dfs -mkdir /user/cloudera/edx/intro-to-apache-spark\n",
    "\n",
    "    - $ hdfs dfs -mkdir /user/cloudera/edx/intro-to-apache-spark/week1\n",
    "\n",
    "    - $ hdfs dfs -put README.md /user/cloudera/edx/intro-to-apache-spark/week1\n",
    "\n",
    "[Hadoop file system shell guide](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df:\n",
      "DataFrame[value: string]\n",
      "[Row(value=u'# Test BerkeleyX: CS105x Introduction to Apache Spark #'), Row(value=u''), Row(value=u'## Welcome to CS 105.1x and Course Objectives/Prequisites ##'), Row(value=u''), Row(value=u'### Requirements ###'), Row(value=u'+ Python 2.7'), Row(value=u'+ PySpark (the Python API for Apache Spark)'), Row(value=u'+ Spark SQL (the SQL API for Apache Spark)'), Row(value=u'+ [Python mini quiz](http://www.mypythonquiz.com/)'), Row(value=u'+ [Python mini course](http://ai.berkeley.edu/tutorial.html#PythonBasics)')]\n"
     ]
    }
   ],
   "source": [
    "df=sqlContext.read.text(\"/user/cloudera/edx/intro-to-apache-spark/week1/README.md\")\n",
    "print(\"df:\\n{}\\n{}\".format(df,df.take(10)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
